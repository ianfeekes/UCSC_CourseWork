# -*- coding: utf-8 -*-
"""Assignment_4_ifeekes_v2_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lo-DcyyYArTzZ4PgZiSU3U7SNwxH8nV0

# CMPS 140 Winter 2019 Assignment 4 Programming Component

## Problem 1

Given the dataset below, implement the logistic regression algorithm, optimizing the parameters using gradient descent and squared error as the loss function. See 18.6.4 from the textbook for details.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_moons

np.random.seed(33)
data, labels = make_moons(n_samples=100, noise=0.1)
colors = ['r' if y else 'b' for y in labels]         #r implies 1, b implies 0 
print(data.shape, labels.shape)
plt.scatter(data[:,0], data[:,1], c=colors)
plt.show()

import math

"""
1 means success with the data set
0 means failure 

We want to create a classifier to predict success or failure 
z=W0+W1Studied+W2Slept
"""

def logistic(x, w):
    h = []                         #initialize as an empty array 
    for i in range(len(x)): 
      h.append(sigmoid(w[0]*x[i][0]+w[1]*x[i][1]+w[2]*x[i][2]))
    return h                       #returns array of h values computed 
    
#Activation function for mapping values between 0 and 1 
def sigmoid(z):
    return 1/(1+math.exp(-z))

def sigmoid_derivative(z):
    return z*(1-z) 
  
#sum for all N (yj-sigmoid(xj))^2
"""
y is a set of labels either 0 or 1 
h is our values computed from logistic regression 
"""
def loss_func(y, h):
    toReturn =0 
    for i in range(len(y)): 
      toReturn+= (y[i]-h[i])*(y[i]-h[i])
    return toReturn 
  

"""
This finds the rate of loss. We don't need to do anything with this 
"""
def loss_func_derivative(y, h, x):
    #print(y)
    #print(h)
    #print(x) 
    #return np.dot((y - h) * sigmoid_derivative(h), x)
    toReturn=0
    for i in range(len(x)): 
      #toReturn+= (y[i]-h[i])*h[i]*(1-h[i])*x[i]
      currVal=0 
      currVal +=np.dot((y[i]-h[i])*sigmoid_derivative(h[i]),x[i])
      toReturn+=currVal 
     
    print("loss derivative is: "+str(toReturn))
    return toReturn 
  
  
"""
m = x.shape[0]
return (1 / m) * np.dot(x.T, sigmoid(net_input(theta,   x)) - y)
"""
    

"""
Takes in labels and heuristics 
"""
def accuracy(y, h):
    toReturn =0                     #a percent value 
    for i in range(len(h)):
      if h[i] ==y[i]: toReturn+=1 
    return toReturn
  
"""
x is data 
y is labels 
learning rate is .5 
Takes 100 steps 

Sets initial weights, includes a bias term in the data, finds accuracy and outputs it
initializes previous loss to 0

for each of the 100 steps, it calculates the h value, and loss value, and sees if 
if has converged. If convergence, then it returns. Else we increment our current
weights with the learning rate and the loss rate. Fairly simple 
"""  
def logistic_regression(x, y, loss_func_derivative, learning_rate, num_steps=100):
    # start with intial parameters w_i = 1
    w = np.ones(3)
    # include a bias term
    x = np.pad(x, [[0,0], [1,0]], mode='constant')
    
    #print("x", x)

    print('Intial Accuracy:{}%'.format(accuracy(y, np.round(logistic(x, w)))))
    prev_loss = 0
    for step in range(num_steps):
        h = logistic(x, w)
        loss = loss_func(y, h)
        if math.fabs(loss - prev_loss) < 0.0000001:
            return w
                
        w = w + learning_rate * loss_func_derivative(y, h, x)
        #print(w) 
        print('Step {} Accuracy:{}%, Loss: {}'.format(step+1, accuracy(y, np.round(logistic(x, w))), loss))
        prev_loss = loss
    return w
    
learning_rate = 0.5
ws = logistic_regression(data, labels, loss_func_derivative, learning_rate)
yh = np.round(logistic(np.pad(data, [[0,0], [1,0]], mode='constant'), ws))
colors = ['g' if _yh==_y else 'r' for _yh, _y in zip(yh, labels.astype(np.int))]
plt.title('Classification Results')
plt.scatter(data[:,0], data[:,1], c=colors)

correct_w = np.array([ 1.        ,  1.15719784, -3.45275268])
print("ws", ws)
print("correct_w", correct_w)
np.testing.assert_allclose(ws, correct_w, rtol=1e-5)

"""## Problem 2

Implement a single perceptron below.

You can assume that both **x** and **w** will be iterables and b will be a single value. All of them will not be *None*.
"""

#since we don't have targets we don't perform learning, we literally just return
#values based on activation...seems too easy 
def perceptron(values, weights, b):
    #w is a set of weights 
    #x is a set of single values 
  activation = b 
  for v,w in values,weights: 
    activation = activation+v*w
  if activation >=0: return 1 
  return 0
    
    
    # raise NotImplementedError()

x = [1, 2]
w = [1, 1]
b = 0
y = perceptron(x, w, b)
print("y", y)
assert y == 1

b = -5
y = perceptron(x, w, b)
print("y", y)
assert y == 0